# -*- coding: utf-8 -*-
"""COMP472Part3ModifiedModelTraining/Evaluation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DAch45zKBHMvYtvG0rtTkjK0lQwNI39k
"""

#Import torch, numpy and sklearn
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
from torchvision.transforms import v2
from torchvision.datasets import ImageFolder
from sklearn.model_selection import train_test_split,KFold
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report,precision_recall_fscore_support
import time
import pandas as pd

!gdown 1aWSSMoNBtDrszbEhn6oLFA2eqhkHRryp
!unzip 'ModifiedCOMP472Data.zip'

#Define the architecture each model used
model = nn.Sequential(
    nn.Conv2d(1, 32, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.BatchNorm2d(32),

    nn.Conv2d(32, 64, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.BatchNorm2d(64),
    nn.MaxPool2d(2, 2),
    nn.Dropout(0.25),

    nn.Conv2d(64, 128, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.BatchNorm2d(128),

    nn.Conv2d(128, 128, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.BatchNorm2d(128),
    nn.MaxPool2d(2, 2),
    nn.Dropout(0.25),

    nn.Conv2d(128, 256, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.BatchNorm2d(256),

    nn.Conv2d(256, 256, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.BatchNorm2d(256),
    nn.MaxPool2d(2, 2),
    nn.Dropout(0.25),

    nn.Flatten(),

    nn.Linear(256 * 6 * 6, 256),
    nn.ReLU(),
    nn.BatchNorm1d(256),
    nn.Dropout(0.5),

    nn.Linear(256, 4)
)

path = '/content/ModifiedCOMP472Data'
transform = v2.Compose([v2.ToImage(),v2.Grayscale(),v2.ToDtype(torch.float32, scale=True)])
full_dataset = ImageFolder(path,transform)
class_names = full_dataset.classes

#Hyperparameters
ttsplit = 0.2 #Train-Test Split %. Currently 80:20
vtsplit = 0.1 #Train-Validation Split %. Currently 90:10 ONLY splitting the traning data
batch_size = 32 #Batch size for model
learning_rate = 0.004083 #Learning rate for the model (step size for SGD)
num_epochs = 20 #Number of epochs to train on

# Set loss function and optimizer, currently CCE for classification and Adam as the optimizer, if you want to change the optimizer: https://pytorch.org/docs/stable/optim.html#algorithms
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Split dataset indices
train_indices, test_indices = train_test_split(list(range(len(full_dataset))), test_size=ttsplit, random_state=42)
train_indices, val_indices = train_test_split(train_indices, test_size=vtsplit, random_state=42)

# Create subsets
train_set = Subset(full_dataset, train_indices)
val_set = Subset(full_dataset, val_indices)
test_set = Subset(full_dataset, test_indices)

# Create data loaders
train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)
val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False)

# Initialize lists to store metrics
train_losses = []
train_accuracies = []
val_losses = []
val_accuracies = []

# Early stopping
best_val_loss = float('inf')
wait = 5  # Number of epochs to wait
counter = 0

# Training the model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Start training time
training_start_time = time.time()

for epoch in range(num_epochs):
    model.train()
    running_loss, correct, total = 0, 0, 0
    epoch_start_time = time.time()

    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    # Calculate training metrics
    train_loss = running_loss / len(train_loader)
    train_accuracy = 100 * correct / total
    train_losses.append(train_loss)
    train_accuracies.append(train_accuracy)

    #training time per epoch
    train_epoch_time = time.time() - epoch_start_time
    print(f'Epoch {epoch + 1} training time: {train_epoch_time:.2f}')

    # Validation
    model.eval()
    valRunning_loss, val_correct, val_total = 0, 0, 0
    val_start = time.time()

    with torch.no_grad():

        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            val_outputs = model(images)
            val_loss = criterion(val_outputs, labels)
            valRunning_loss += val_loss.item()
            _, val_predicted = torch.max(val_outputs.data, 1)
            val_total += labels.size(0)
            val_correct += (val_predicted == labels).sum().item()

    # Calculate validation metrics
    val_loss = valRunning_loss / len(val_loader)
    val_accuracy = 100 * val_correct / val_total
    val_losses.append(val_loss)
    val_accuracies.append(val_accuracy)

    # Early stopping check
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        counter = 0  # Reset counter
        torch.save(model.state_dict(), 'modifiedDatasetbestModel.pth')  # Save the best model
    else:
        counter += 1  # Increase counter

    if counter >= wait:
        print("Early stopping")
        break

    #validation metrics
    val_epoch_time = time.time() - val_start
    print(f'Epoch {epoch + 1} validation time: {val_epoch_time:.2f}')
    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')

# training time
print("Total: %s seconds " % (time.time() - training_start_time))

#Method to get the predicted and true labels for each model
def evaluate(model,loader):
    model.eval()
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for inputs, labels in loader:
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    return np.array(all_preds), np.array(all_labels)

model = nn.Sequential(
  nn.Conv2d(1, 32, kernel_size=3, padding=1),
  nn.ReLU(),
  nn.BatchNorm2d(32),

  nn.Conv2d(32, 64, kernel_size=3, padding=1),
  nn.ReLU(),
  nn.BatchNorm2d(64),
  nn.MaxPool2d(2, 2),
  nn.Dropout(0.25),

  nn.Conv2d(64, 128, kernel_size=3, padding=1),
  nn.ReLU(),
  nn.BatchNorm2d(128),

  nn.Conv2d(128, 128, kernel_size=3, padding=1),
  nn.ReLU(),
  nn.BatchNorm2d(128),
  nn.MaxPool2d(2, 2),
  nn.Dropout(0.25),

  nn.Conv2d(128, 256, kernel_size=3, padding=1),
  nn.ReLU(),
  nn.BatchNorm2d(256),

  nn.Conv2d(256, 256, kernel_size=3, padding=1),
  nn.ReLU(),
  nn.BatchNorm2d(256),
  nn.MaxPool2d(2, 2),
  nn.Dropout(0.25),

  nn.Flatten(),

  nn.Linear(256 * 6 * 6, 256),
  nn.ReLU(),
  nn.BatchNorm1d(256),
  nn.Dropout(0.5),

  nn.Linear(256, 4)
  )

model.load_state_dict(torch.load('/content/modifiedDatasetbestModel.pth', map_location=torch.device('cpu')))

age_gender = pd.read_excel('/content/labeled_data/modified_all_labels.xlsx')

print(age_gender['Gender'].value_counts())
print(age_gender['Age'].value_counts())
print(age_gender['Gender'].value_counts(normalize=True))
print(age_gender['Age'].value_counts(normalize=True))

bias_loader = torch.utils.data.DataLoader(full_dataset, batch_size=1, shuffle=False)
bias_preds, bias_labels = evaluate(model,bias_loader)

#Display the confusion matrix
cm = confusion_matrix(bias_labels,bias_preds)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot()

#Display the classification report
print("Main Model Classification Report:")
print(classification_report(bias_labels, bias_preds, target_names=class_names))
print("Main Model Macro/Micro")
macro_p,macro_r,macro_f,_ = precision_recall_fscore_support(bias_labels,bias_preds,average='macro')
micro_p,micro_r,micro_f,_ = precision_recall_fscore_support(bias_labels,bias_preds,average='micro')
print("Macro: " + str(round(macro_p,3)) + " " + str(round(macro_r,3)) + " " + str(round(macro_f,3)))
print("Micro: " + str(round(micro_p,3)) + " " + str(round(micro_r,3)) + " " + str(round(micro_f,3)))

mismatch_indices = [i for i, (pred, true) in enumerate(zip(bias_preds, bias_labels)) if pred != true]
mismatched_rows = age_gender.iloc[mismatch_indices]
print(mismatched_rows['Gender'].value_counts())
print(mismatched_rows['Age'].value_counts())
print(mismatched_rows['Gender'].value_counts(normalize=True))
print(mismatched_rows['Age'].value_counts(normalize=True))

age_indices = age_gender.groupby('Age').groups
gender_indices = age_gender.groupby('Gender').groups

young_idx = age_indices['Young']
middle_idx = age_indices['Middle-aged']
senior_idx = age_indices['Senior']
female_idx = gender_indices['Female']
male_idx = gender_indices['Male']
other_idx = gender_indices['Other/Non-binary']

idxs = [young_idx,middle_idx,senior_idx,female_idx,male_idx,other_idx]

def bias_eval(model,indicies,dataset):
  class_names = dataset.classes
  test_set = Subset(dataset, indicies)
  loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=False)
  preds,true = evaluate(model,loader)
  print(classification_report(true, preds, target_names=class_names))

groups = ['young','middle','senior','female','male','other/non_binary']
for i,indicies in enumerate(idxs):
  print(f'{groups[i]}')
  bias_eval(model,indicies,full_dataset)