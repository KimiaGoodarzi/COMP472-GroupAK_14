# -*- coding: utf-8 -*-
"""ModifiedModelKFold

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L_uvI_Egl8Bdtc-yOCNdB150rlOMxmmN
"""

#Import torch, numpy and sklearn
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
from torchvision.transforms import v2
from torchvision.datasets import ImageFolder
from sklearn.model_selection import train_test_split,KFold
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report,precision_recall_fscore_support
import time
import pandas as pd

!gdown 1aWSSMoNBtDrszbEhn6oLFA2eqhkHRryp
!unzip 'ModifiedCOMP472Data.zip'

#Define the architecture each model used
model = nn.Sequential(
    nn.Conv2d(1, 32, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.BatchNorm2d(32),

    nn.Conv2d(32, 64, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.BatchNorm2d(64),
    nn.MaxPool2d(2, 2),
    nn.Dropout(0.25),

    nn.Conv2d(64, 128, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.BatchNorm2d(128),

    nn.Conv2d(128, 128, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.BatchNorm2d(128),
    nn.MaxPool2d(2, 2),
    nn.Dropout(0.25),

    nn.Conv2d(128, 256, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.BatchNorm2d(256),

    nn.Conv2d(256, 256, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.BatchNorm2d(256),
    nn.MaxPool2d(2, 2),
    nn.Dropout(0.25),

    nn.Flatten(),

    nn.Linear(256 * 6 * 6, 256),
    nn.ReLU(),
    nn.BatchNorm1d(256),
    nn.Dropout(0.5),

    nn.Linear(256, 4)
)

#Method to get the predicted and true labels for each model
def evaluate(model,loader):
    model.eval()
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for inputs, labels in loader:
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    return np.array(all_preds), np.array(all_labels)

#Hyperparameters
batch_size = 32 #Batch size for model
learning_rate = 0.004083 #Learning rate for the model (step size for SGD)
num_epochs = 20 #Number of epochs to train on

# Set loss function and optimizer, currently CCE for classification and Adam as the optimizer, if you want to change the optimizer: https://pytorch.org/docs/stable/optim.html#algorithms
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

path = '/content/ModifiedCOMP472Data'
transform = v2.Compose([v2.ToImage(),v2.Grayscale(),v2.ToDtype(torch.float32, scale=True)])
full_dataset = ImageFolder(path,transform)
class_names = full_dataset.classes

kf = KFold(n_splits=10, random_state=42, shuffle=True)

macroavg = [0,0,0]
microavg = [0,0,0]
accavg = 0

for i, (train_index, test_index) in enumerate(kf.split(full_dataset)):

  train_set = Subset(full_dataset, train_index)
  test_set = Subset(full_dataset, test_index)
  train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)
  val_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)

  model = nn.Sequential(
    nn.Conv2d(1, 32, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.BatchNorm2d(32),

    nn.Conv2d(32, 64, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.BatchNorm2d(64),
    nn.MaxPool2d(2, 2),
    nn.Dropout(0.25),

    nn.Conv2d(64, 128, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.BatchNorm2d(128),

    nn.Conv2d(128, 128, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.BatchNorm2d(128),
    nn.MaxPool2d(2, 2),
    nn.Dropout(0.25),

    nn.Conv2d(128, 256, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.BatchNorm2d(256),

    nn.Conv2d(256, 256, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.BatchNorm2d(256),
    nn.MaxPool2d(2, 2),
    nn.Dropout(0.25),

    nn.Flatten(),

    nn.Linear(256 * 6 * 6, 256),
    nn.ReLU(),
    nn.BatchNorm1d(256),
    nn.Dropout(0.5),

    nn.Linear(256, 4)
  )

  criterion = nn.CrossEntropyLoss()
  optimizer = optim.Adam(model.parameters(), lr=learning_rate)
  #
  preds = []
  true = []
  # Initialize lists to store metrics
  train_losses = []
  train_accuracies = []
  val_losses = []
  val_accuracies = []

  # Early stopping
  best_val_loss = float('inf')
  wait = 5  # Number of epochs to wait
  counter = 0

  # Training the model
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model.to(device)

  # Start training time
  training_start_time = time.time()

  for epoch in range(num_epochs):
      model.train()
      running_loss, correct, total = 0, 0, 0
      epoch_start_time = time.time()

      for images, labels in train_loader:
          images, labels = images.to(device), labels.to(device)
          optimizer.zero_grad()
          outputs = model(images)
          loss = criterion(outputs, labels)
          loss.backward()
          optimizer.step()
          running_loss += loss.item()
          _, predicted = torch.max(outputs.data, 1)
          total += labels.size(0)
          correct += (predicted == labels).sum().item()

      # Calculate training metrics
      train_loss = running_loss / len(train_loader)
      train_accuracy = 100 * correct / total
      train_losses.append(train_loss)
      train_accuracies.append(train_accuracy)

      #training time per epoch
      train_epoch_time = time.time() - epoch_start_time
      #print(f'Epoch {epoch + 1} training time: {train_epoch_time:.2f}')

      # Validation
      model.eval()
      valRunning_loss, val_correct, val_total = 0, 0, 0
      val_start = time.time()

      with torch.no_grad():

          for images, labels in val_loader:
              images, labels = images.to(device), labels.to(device)
              val_outputs = model(images)
              val_loss = criterion(val_outputs, labels)
              valRunning_loss += val_loss.item()
              _, val_predicted = torch.max(val_outputs.data, 1)
              val_total += labels.size(0)
              val_correct += (val_predicted == labels).sum().item()
              true.extend(labels.cpu().detach().numpy())
              preds.extend(val_predicted.cpu().detach().numpy())
      # Calculate validation metrics
      val_loss = valRunning_loss / len(val_loader)
      val_accuracy = 100 * val_correct / val_total
      val_losses.append(val_loss)
      val_accuracies.append(val_accuracy)

      # Early stopping check
      if val_loss < best_val_loss:
          best_val_loss = val_loss
          best_val_accuracy = val_accuracy
          counter = 0  # Reset counter
          trues = np.array(true)
          predictions = np.array(preds)
          cm = confusion_matrix(trues,predictions)
          macro_p,macro_r,macro_f,_ = precision_recall_fscore_support(trues,predictions,average='macro')
          micro_p,micro_r,micro_f,_ = precision_recall_fscore_support(trues,predictions,average='micro')
          #torch.save(model.state_dict(), f'bestkfoldmodel{i}.pth')  # Save the best model
      else:
          counter += 1  # Increase counter

      if counter >= wait:
          print("Early stopping")
          print(f'kfold{i+1}')
          print(f"accuracy {best_val_accuracy}")
          print("Macro: " + str(round(macro_p,3)) + " " + str(round(macro_r,3)) + " " + str(round(macro_f,3)))
          print("Micro: " + str(round(micro_p,3)) + " " + str(round(micro_r,3)) + " " + str(round(micro_f,3)))

          macroavg[0] += macro_p
          macroavg[1] += macro_r
          macroavg[2] += macro_f

          microavg[0] += micro_p
          microavg[1] += micro_r
          microavg[2] += micro_f

          accavg += best_val_accuracy
          disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
          disp.plot()
          break

      #validation metrics
      val_epoch_time = time.time() - val_start
      #print(f'Epoch {epoch + 1} validation time: {val_epoch_time:.2f}')
      #print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')

  # training time
  #print("Total: %s seconds " % (time.time() - training_start_time))


print('Average Scores:')
print(f'accuracy: {accavg/10}')
print(f'macro: {macroavg[0]/10}  {macroavg[1]/10}  {macroavg[2]/10}')
print(f'micro: {microavg[0]/10}  {microavg[1]/10}  {microavg[2]/10}')